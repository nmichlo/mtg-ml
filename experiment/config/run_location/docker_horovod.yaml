# @package _global_
defaults:
#  - override /hydra/job_logging: colorlog
#  - override /hydra/hydra_logging: colorlog
  - override /hydra/launcher: horovod

# correctly set the working directories for hydra
# - every time an experiment is run we increment the current dir
hydra:
  job:
    env_set:
      # ----------------- #
      # PROGRAM VARS
      ML_DATA_ROOT: /data
      ML_OUT_ROOT: /workspace/out
      # ----------------- #
      # DEBUG VARS: https://pytorch.org/docs/stable/distributed.html
      NCCL_DEBUG: "INFO"
      NCCL_BLOCKING_WAIT: "1"
      TORCH_DISTRIBUTED_DEBUG: "DETAIL"
      PL_TORCH_DISTRIBUTED_BACKEND: "nccl"
      # NCCL_DEBUG_SUBSYS: "ALL"
      # NCCL_ASYNC_ERROR_HANDLING: "1"
      # CUDA_LAUNCH_BLOCKING: "1"
      # ----------------- #
  run:
    dir: ${hydra.job.env_set.ML_OUT_ROOT}/${next_num:${hydra.job.env_set.ML_OUT_ROOT}}__${now:%Y-%m-%d--%H-%M-%S}
  sweep:
    dir: ${hydra.job.env_set.ML_OUT_ROOT}/${next_num:${hydra.job.env_set.ML_OUT_ROOT}}__${now:%Y-%m-%d--%H-%M-%S}
    subdir: ${hydra.job.num}

# override the trainer strategy
trainer:
  gpus: 1
  strategy: "horovod"
